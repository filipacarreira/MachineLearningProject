DECISION TREE: 
    dt = DecisionTreeClassifier(splitter = 'random', min_samples_split=450,class_weight={0:1,1:5.6}, criterion='gini',random_state=42).fit(X_train_eng[set1], y_train_df)
    F1 Train: 0.6583097452486859
    F1 Val: 0.6725490196078431
    ROC: 0.8510632762089779
    K-FOLD: F1 score: 0.643
            ROC_AUC score: 0.841 
    
RANDOM FOREST kaggle=0.676
clf=RandomForestClassifier(n_estimators=20,class_weight='balanced_subsample',criterion="gini", max_features=None, min_samples_leaf=450,random_state=42).fit(X_train_eng[sets_RF1], y_train_df)
    F1 Train: 0.6591276252019386
    F1 Val: 0.6718903036238981
    ROC: 0.8508580216933787
    K-FOLD: F1 score: 0.660
            ROC_AUC score: 0.847

SVM: KAGGLE: 0.6701
class_weight='balanced',C= 10, gamma= 0.1, kernel='rbf',random_state=42 s
	Set1_no_bin
	F1 Train: 0.6590257879656162
	F1 Val: 0.6760000000000002
	ROC: 0.8482735493420293

LOGISTIC REGRESSION: Kaggle = 0.67841
LogisticRegression(C = 0.01, penalty = 'l2', solver = 'newton-cg', class_weight='balanced').fit(X_train_out_robust[set1_no_bin], y_train_out)
	F1 Train: 0.66721446179129
	F1 Val: 0.6589595375722543
	ROC: 0.830860661141736